'''
Collect requirements to source files mappings
This program gets a set of bug ids from a csv file and attaches those ids at the end of bugzilla website to
gather information about each bug ids.
'''

import pandas as pd
from bs4 import BeautifulSoup
import requests
from time import sleep

URL_PREFIX = 'https://bugzilla.mozilla.org/show_bug.cgi?id={}'

df = pd.read_csv('bugid2commit.csv')

# arrays to keep the data
open_dates = []
close_dates = []
titles = []
products = []
bug_types = []
components =[]
fixed_versions = []
assignees = []
descriptions = []
reporters = []
statuss = []

def scrape(url):
    """
    scraping firefox bug details from Bugzilla
    :param url: bug url
    :return: list of bug details
    """

    open_date = None
    close_date = None
    title = None
    product = None
    bug_type = None
    component = None
    version = None
    assignee = None
    description = None
    reporter = None
    status = None

    print('scraping %s' % url)
    try:
        # save all the content from the website into variable soup
        soup = BeautifulSoup(requests.get(url).content, 'lxml')
        if soup.find(id='docslinks') is not None or \
                'FIXED' not in soup.find(id='field-value-status-view').text:
            return [None] * 6

        title = soup.find('h1', id='field-value-short_desc').text
        date = soup.find('div', id='field-status_summary')
        '''
        find all the occurrence of class 'rel-time' because both open and close dates are stored in
        class 'rel-time' in the website where opened date occurs before closed.
        There is no way to distinguish them directly
        '''
        ct = date.findAll('span', class_='rel-time')
        open_date = ct[0].get('title')[:16] # it is limited to 10 characters because date is 10 characters at most
        close_date = ct[1].get('title')[:16]
        # encode('utf-8') is used to encode any unicode characters that is not exportable to csv file
        # it encodes any such characters to some ascii characters
        product = soup.find('div', id='field-product').find('span', id='product-name').text.encode('utf-8').strip()
        bug_type = soup.find('div', id='field-bug_type').find('span', class_='bug-type-label iconic-text').get('data-type')
        component = soup.find('div', id='field-component').find('span', id='component-name').text.encode('utf-8').strip()
        status = soup.find('div', id='field-status-view').find('span', id='field-value-status-view').text.strip()
        version = soup.find('div', id='field-target_milestone').find('span', id='field-value-target_milestone').text.strip()
        assignee = soup.find('div', id='field-assigned_to').find('a', class_='email').find('span', class_='fna').text
        reporter = soup.find('div', id='field-reporter').find('a', class_='email').find('span', class_='fna').text
        description = soup.find('pre', id='ct-0').text.encode('utf-8').strip()

        sleep(1)

    except Exception as e:
        print(e)


    return [title, open_date, close_date, product, bug_type, component, status, version, assignee, reporter, description]


for i, id in enumerate(df.id):
    print(str(i))
    url = URL_PREFIX.format(id)
    results = scrape(url)
    # append extracted data to arrays
    titles.append(results[0])
    open_dates.append(results[1])
    close_dates.append(results[2])
    products.append(results[3])
    bug_types.append(results[4])
    components.append(results[5])
    statuss.append(results[6])
    fixed_versions.append(results[7])
    assignees.append(results[8])
    reporters.append(results[9])
    descriptions.append(results[10])


# export arrays to respective titles in csv file
df['Title'] = pd.Series(titles, index=df.index)
df['Open date/Time'] = pd.Series(open_dates, index=df.index)
df['Close date/Time'] = pd.Series(close_dates, index=df.index)
df['Product'] = pd.Series(products, index=df.index)
df['Type'] = pd.Series(bug_types, index=df.index)
df['Component'] = pd.Series(components, index=df.index)
df['Status'] = pd.Series(statuss, index=df.index)
df['Version'] = pd.Series(fixed_versions, index=df.index)
df['Assignee'] = pd.Series(assignees, index=df.index)
df['Reporter'] = pd.Series(reporters, index=df.index)
df['Description'] = pd.Series(descriptions, index=df.index)

# create and export all the data to csv file
df.to_csv('req2source_2.csv', index=False)